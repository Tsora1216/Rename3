
train_record_dfには、訓練用メタデータ( train_records.csv )でリンク先は(https://s3.ap-northeast-1.amazonaws.com/nishika.assets.private/competitions/39/data/train_records.csv?response-content-disposition=attachment%3B%20filename%3Dtrain_records.csv&AWSAccessKeyId=ASIA3NMWWMCVQCB7RXBC&Signature=sQa6pa9SqoNZ03PHadFuQwLxqoA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEAcaDmFwLW5vcnRoZWFzdC0xIkcwRQIgNBHCd%2Bfn1q%2FygfKTaZ0fAp6YtdUSLH%2BcnApmubRXjJECIQDb3MZ3WjqAbG%2FWrlA68yjKZpNX7WIxzcBAAva2LJOdHiqCBAgwEAMaDDc4NDY4NDQ0MTc3MSIMSyHE3%2BLHYWlJf7S5Kt8Djlvj9%2FrC2FUrJ%2FmigWkH3lplzJ%2B%2B8b1xSzZPMi4f3%2F8v41AluEDp9%2BEz9hvk%2FXdFCtVjsa2mcOpyWsF1RucWXh8mOKGF37jtS64RRvDrf5hHy7Z%2B9%2BCYuHPx%2F6fsv5jfhAqVWgN0EnfBN5k4mXP1Rilyh0%2FPPAOoBL2oYI7n7W2UwTvWJ%2BpQ48WugsbwUGCansXnjUETgxaEt5zi0BqkoSCnXf83H6n8990ZIfKktxSeBG8ynQoS5E7zIazphigj9tCJasmsY9DPef84rIgoTrgycf74DTGc5ngf%2BkLy25kzBnx4Y45PwEwCIHgRUAeRwT38FxyqU44Kg%2BbW677m1neQupFFjb4h0q9JvY7DKHEa1knvDCjhABuMJM5WNpjvNARtexdEQVfYSQYvVzMBNWrxOp0qEVn8rB9ZkkXOs55ljPkh09FQoqe47sxO6yz31RNvrPExsIPrdBXZZmluKtuN6URcQNePuSC5hGJlo0Dg2uVAgOR2%2FT4TOFUQNzow7BDw%2FUVDe5N%2BjTpqjKTRyK%2B4Yq%2FkQkzryeUqwcYUiBGLXh6gOrkBguXtmn8JkbR4G%2FZ3SlcRpoDhMFTjHSqYDO1oHmbPuOrxvscljv%2B%2F0VPVvEi85Oyewk%2FU0xGPVHUwirqOowY6pQEpUglz2I0BhV9%2BbLFrIwQ4a50%2B5u%2BaDMolSPboubBXIdj1OQeupBQDce%2BKyOe95RQcT%2BMRa4xuwwTQO%2Fcl1RzBrws7OhqxsT3J2N3pUIa%2F0svuD97mjzF3G9N19w4ZiJpRsrERhi05lWO%2Fl61ySo4Umy1o2RQOAxitlbWP852LdQpyqE%2F8MNWrxQk3lG0Zd3kqto3q0%2FGGyuoTMADxMQ1gw8QGosY%3D&Expires=1684269847)のCSVファイルが入っており、
test_record_dfには、テスト用メタデータ( test_records.csv )でリンク先は(https://s3.ap-northeast-1.amazonaws.com/nishika.assets.private/competitions/39/data/test_records.csv?response-content-disposition=attachment%3B%20filename%3Dtest_records.csv&AWSAccessKeyId=ASIA3NMWWMCVQCB7RXBC&Signature=yibHx3HvgkLSH%2FHgw8mFKRfHX4w%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEAcaDmFwLW5vcnRoZWFzdC0xIkcwRQIgNBHCd%2Bfn1q%2FygfKTaZ0fAp6YtdUSLH%2BcnApmubRXjJECIQDb3MZ3WjqAbG%2FWrlA68yjKZpNX7WIxzcBAAva2LJOdHiqCBAgwEAMaDDc4NDY4NDQ0MTc3MSIMSyHE3%2BLHYWlJf7S5Kt8Djlvj9%2FrC2FUrJ%2FmigWkH3lplzJ%2B%2B8b1xSzZPMi4f3%2F8v41AluEDp9%2BEz9hvk%2FXdFCtVjsa2mcOpyWsF1RucWXh8mOKGF37jtS64RRvDrf5hHy7Z%2B9%2BCYuHPx%2F6fsv5jfhAqVWgN0EnfBN5k4mXP1Rilyh0%2FPPAOoBL2oYI7n7W2UwTvWJ%2BpQ48WugsbwUGCansXnjUETgxaEt5zi0BqkoSCnXf83H6n8990ZIfKktxSeBG8ynQoS5E7zIazphigj9tCJasmsY9DPef84rIgoTrgycf74DTGc5ngf%2BkLy25kzBnx4Y45PwEwCIHgRUAeRwT38FxyqU44Kg%2BbW677m1neQupFFjb4h0q9JvY7DKHEa1knvDCjhABuMJM5WNpjvNARtexdEQVfYSQYvVzMBNWrxOp0qEVn8rB9ZkkXOs55ljPkh09FQoqe47sxO6yz31RNvrPExsIPrdBXZZmluKtuN6URcQNePuSC5hGJlo0Dg2uVAgOR2%2FT4TOFUQNzow7BDw%2FUVDe5N%2BjTpqjKTRyK%2B4Yq%2FkQkzryeUqwcYUiBGLXh6gOrkBguXtmn8JkbR4G%2FZ3SlcRpoDhMFTjHSqYDO1oHmbPuOrxvscljv%2B%2F0VPVvEi85Oyewk%2FU0xGPVHUwirqOowY6pQEpUglz2I0BhV9%2BbLFrIwQ4a50%2B5u%2BaDMolSPboubBXIdj1OQeupBQDce%2BKyOe95RQcT%2BMRa4xuwwTQO%2Fcl1RzBrws7OhqxsT3J2N3pUIa%2F0svuD97mjzF3G9N19w4ZiJpRsrERhi05lWO%2Fl61ySo4Umy1o2RQOAxitlbWP852LdQpyqE%2F8MNWrxQk3lG0Zd3kqto3q0%2FGGyuoTMADxMQ1gw8QGosY%3D&Expires=1684269847)のCSVファイルが入っている。
PSGが保存されているEDFデータ( edf_data.tar.gz )

途中で使用しているPSGのデータは、このリンク（https://s3.ap-northeast-1.amazonaws.com/nishika.assets.private/competitions/39/data/edf_data.tar.gz?response-content-disposition=attachment%3B%20filename%3Dedf_data.tar.gz&AWSAccessKeyId=ASIA3NMWWMCVQCB7RXBC&Signature=4lXnypITg17GnZYAim2get4WTIA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEAcaDmFwLW5vcnRoZWFzdC0xIkcwRQIgNBHCd%2Bfn1q%2FygfKTaZ0fAp6YtdUSLH%2BcnApmubRXjJECIQDb3MZ3WjqAbG%2FWrlA68yjKZpNX7WIxzcBAAva2LJOdHiqCBAgwEAMaDDc4NDY4NDQ0MTc3MSIMSyHE3%2BLHYWlJf7S5Kt8Djlvj9%2FrC2FUrJ%2FmigWkH3lplzJ%2B%2B8b1xSzZPMi4f3%2F8v41AluEDp9%2BEz9hvk%2FXdFCtVjsa2mcOpyWsF1RucWXh8mOKGF37jtS64RRvDrf5hHy7Z%2B9%2BCYuHPx%2F6fsv5jfhAqVWgN0EnfBN5k4mXP1Rilyh0%2FPPAOoBL2oYI7n7W2UwTvWJ%2BpQ48WugsbwUGCansXnjUETgxaEt5zi0BqkoSCnXf83H6n8990ZIfKktxSeBG8ynQoS5E7zIazphigj9tCJasmsY9DPef84rIgoTrgycf74DTGc5ngf%2BkLy25kzBnx4Y45PwEwCIHgRUAeRwT38FxyqU44Kg%2BbW677m1neQupFFjb4h0q9JvY7DKHEa1knvDCjhABuMJM5WNpjvNARtexdEQVfYSQYvVzMBNWrxOp0qEVn8rB9ZkkXOs55ljPkh09FQoqe47sxO6yz31RNvrPExsIPrdBXZZmluKtuN6URcQNePuSC5hGJlo0Dg2uVAgOR2%2FT4TOFUQNzow7BDw%2FUVDe5N%2BjTpqjKTRyK%2B4Yq%2FkQkzryeUqwcYUiBGLXh6gOrkBguXtmn8JkbR4G%2FZ3SlcRpoDhMFTjHSqYDO1oHmbPuOrxvscljv%2B%2F0VPVvEi85Oyewk%2FU0xGPVHUwirqOowY6pQEpUglz2I0BhV9%2BbLFrIwQ4a50%2B5u%2BaDMolSPboubBXIdj1OQeupBQDce%2BKyOe95RQcT%2BMRa4xuwwTQO%2Fcl1RzBrws7OhqxsT3J2N3pUIa%2F0svuD97mjzF3G9N19w4ZiJpRsrERhi05lWO%2Fl61ySo4Umy1o2RQOAxitlbWP852LdQpyqE%2F8MNWrxQk3lG0Zd3kqto3q0%2FGGyuoTMADxMQ1gw8QGosY%3D&Expires=1684269847）のものである。

これらのデータから下記の条件で特長量を生成したい。

生成する際は、リンク先（https://paperswithcode.com/paper/do-not-sleep-on-linear-models-simple-and）を確認し、詳細な定義を学習してから行うこと
脳波の周波数スペクトル。これは、脳波の周波数成分の強度をグラフで表したものであり、睡眠の段階を区別するのに役立つ。
脳波のエントロピー。これは、脳波の乱雑さの尺度であり、睡眠の深さを区別するのに役立つ。
心拍数。これは、心臓が1分間に拍動する回数であり、睡眠の質を区別するのに役立つ。
呼吸数。これは、1分間に呼吸する回数であり、睡眠の質を区別するのに役立つ。
体動。これは、睡眠中に身体を動かす回数であり、睡眠の質を区別するのに役立つ。
体温。これは、睡眠中の体温であり、睡眠の質を区別するのに役立つ。
血中酸素飽和度。これは、血液中の酸素の量であり、睡眠の質を区別するのに役立つ。
睡眠ポリグラフ（PSG）の特徴量。これは、EEG、ECG、EMG、EOGなどのさまざまな生体測定値を組み合わせたものです。PSGは、睡眠の段階と質を詳細に分析するのに役立ちます。
睡眠時間：これは、1晩の睡眠時間（時間）です。
睡眠効率：これは、実際に眠っている時間の割合（％）です。
レム睡眠の割合：これは、レム睡眠の割合（％）です。
ノンレム睡眠の割合：これは、ノンレム睡眠の割合（％）です。
浅いノンレム睡眠の割合：これは、浅いノンレム睡眠の割合（％）です。
深いノンレム睡眠の割合：これは、深いノンレム睡眠の割合（％）です。
覚醒の回数：これは、睡眠中に目覚める回数です。
覚醒の長さ：これは、各覚醒の長さ（時間）です。
睡眠の質：これは、睡眠の質の尺度（1から5までのスケール）です。
日中の眠気：これは、日中の眠気の尺度（1から5までのスケール）です。
気分：これは、睡眠後の気分の尺度（1から5までのスケール）です。
https://github.com/emadeldeen24/AttnSleepを再学習させ推論結果を特徴量に追加

それらのデータは下記のプログラムに追加する場合どのようなPythonのコードになるか教えてください

import datetime
from pathlib import Path
from typing import Dict, List
import pandas as pd
import numpy as np
import warnings
from tqdm.auto import tqdm
from sklearn.model_selection import GroupKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score
import mne
import joblib
import plotly.graph_objects as go

OUTPUT_NAME = "submit_rf_v1"
OUTPUT_DIR = Path("./output/"+OUTPUT_NAME+"_0.00.csv")
DATA_DIR = Path("./input/")
EDF_DIR = DATA_DIR / "edf_data"
MODEL_DIR = Path("../pickle/"+"Telecommunications_Project-"+OUTPUT_NAME+"_model.pkl")


sample_submission_df = pd.read_csv(DATA_DIR/"sample_submission.csv", parse_dates=[1])
sample_submission_df
train_record_df = pd.read_csv(DATA_DIR/"train_records.csv")
test_record_df = pd.read_csv(DATA_DIR/"test_records.csv")
train_record_df.head()
test_record_df.head()

len(set(train_record_df["subject_id"].unique()) & set(test_record_df["subject_id"].unique()))
print("訓練被験者数", len(train_record_df["subject_id"].unique()))
print("テスト被験者数", len(test_record_df["subject_id"].unique()))

train_record_df["hypnogram"] = train_record_df["hypnogram"].map(lambda x: str(EDF_DIR/x))
train_record_df["psg"] = train_record_df["psg"].map(lambda x: str(EDF_DIR/x))
test_record_df["psg"] = test_record_df["psg"].map(lambda x: str(EDF_DIR/x))
row = train_record_df.iloc[0]

psg_edf = mne.io.read_raw_edf(row["psg"], preload=False)
type(psg_edf)
psg_edf.info
psg_edf.ch_names
psg_df = psg_edf.to_data_frame()
print(psg_df)
meas_start = psg_edf.info["meas_date"]
meas_start = meas_start.replace(tzinfo=None)
psg_df["meas_time"] = pd.date_range(start=meas_start, periods=len(psg_df), freq=pd.Timedelta(1 / 100, unit="s"))
print(psg_df)

annot = mne.read_annotations(row["hypnogram"])
annot_df = annot.to_data_frame()
print(annot_df)
annot_df["description"].value_counts()
psg_edf = mne.io.read_raw_edf(row["psg"], include=["EEG Fpz-Cz"], verbose=False)
annot = mne.read_annotations(row["hypnogram"])

truncate_start_point = 3600 * 5
truncate_end_point = (len(psg_edf)/100) - (3600 *5)
annot.crop(truncate_start_point, truncate_end_point, verbose=False)
psg_edf.set_annotations(annot, verbose=False, emit_warning=False)

RANDK_LABEL2ID = {
    'Movement time': -1,
    'Sleep stage ?': -1,
    'Sleep stage W': 0,
    'Sleep stage 1': 1,
    'Sleep stage 2': 2,
    'Sleep stage 3': 3,
    'Sleep stage 4': 3,
    'Sleep stage R': 4
}
events, _ = mne.events_from_annotations(psg_edf, event_id=RANDK_LABEL2ID, chunk_duration=30., verbose=False)
LABEL2ID = {
    'Movement time': -1,
    'Sleep stage ?': -1,
    'Sleep stage W': 0,
    'Sleep stage 1': 1,
    'Sleep stage 2': 2,
    'Sleep stage 3/4': 3,
    'Sleep stage R': 4
}
tmax = 30. - 1. / psg_edf.info['sfreq']
epoch = mne.Epochs(raw=psg_edf, events=events, event_id=LABEL2ID, tmin=0, tmax=tmax, baseline=None, verbose=False, on_missing='ignore')

epoch.info["temp"] = {
    "id":row["id"],
    "subject_id":row["subject_id"],
    "night":row["night"],
    "truncate_start_point":truncate_start_point,
    "truncate_end_point":truncate_end_point
}
print(type(epoch))
print(epoch)
epoch_df = epoch.to_data_frame(verbose=False)
print(epoch_df)
print(events)
print(events.shape)
new_meas_date = epoch.info["meas_date"].replace(tzinfo=None) + datetime.timedelta(seconds=epoch.info["temp"]["truncate_start_point"])

epoch_df["meas_time"] = pd.date_range(start=new_meas_date, periods=len(epoch_df), freq=pd.Timedelta(1 / 100, unit="s"))

print(epoch_df)

def epoch_to_df(epoch:mne.epochs.Epochs) -> pd.DataFrame:
    truncate_start_point = epoch.info["temp"]["truncate_start_point"]
    df = epoch.to_data_frame(verbose=False)
    new_meas_date = epoch.info["meas_date"].replace(tzinfo=None) + datetime.timedelta(seconds=truncate_start_point)
    df["meas_time"] = pd.date_range(start=new_meas_date, periods=len(df), freq=pd.Timedelta(1 / 100, unit="s"))
    return df

epoch_to_df(epoch)

label_df = epoch_df.loc[epoch_df.groupby("epoch")["time"].idxmin()][["meas_time"]].reset_index(drop=True)
label_df["condition"] = "Sleep stage W"
label_df["id"] = epoch.info["temp"]["id"]
label_df

def epoch_to_sub_df(epoch_df:pd.DataFrame, id, is_train:bool) -> pd.DataFrame:
    cols = ["id", "meas_time"]
    if is_train:
        cols.append("condition")
    label_df = epoch_df.loc[epoch_df.groupby("epoch")["time"].idxmin()].reset_index(drop=True)
    label_df["id"] = id
    return label_df[cols]

epoch_to_sub_df(epoch_df, epoch.info["temp"]["id"], is_train=True)
test_row = test_record_df.iloc[0]
psg_edf = mne.io.read_raw_edf(test_row["psg"], include=["EEG Fpz-Cz"], verbose=False)

start_psg_date = psg_edf.info["meas_date"]
start_psg_date = start_psg_date.replace(tzinfo=None)

test_start_time = sample_submission_df[sample_submission_df["id"]==test_row["id"]]["meas_time"].min()
test_end_time = sample_submission_df[sample_submission_df["id"]==test_row["id"]]["meas_time"].max()
print(f"psg start: {start_psg_date},  test start: {test_start_time}, test end: {test_end_time}")

truncate_start_point = int((test_start_time - start_psg_date).total_seconds())
truncate_end_point = int((test_end_time - start_psg_date).total_seconds())+30
print(f"event start sencond: {truncate_start_point}, event end second: {truncate_end_point} ")

event_range = list(range(truncate_start_point, truncate_end_point, 30))
events = np.zeros((len(event_range), 3), dtype=int)
events[:, 0] = event_range

events = events * 100

tmax = 30. - 1. / psg_edf.info['sfreq']
epoch = mne.Epochs(raw=psg_edf, events=events, event_id={'Sleep stage W': 0}, tmin=0, tmax=tmax, baseline=None, verbose=False)

epoch.to_data_frame()

def read_and_set_annoation(record_df:pd.DataFrame, include=None, is_test=False) -> List[mne.epochs.Epochs]:
    whole_epoch_data = []
    for row_id, row in tqdm(record_df.iterrows(), total=len(record_df)):
        psg_edf = mne.io.read_raw_edf(row["psg"], include=include, verbose=False)
        if not is_test:
            annot = mne.read_annotations(row["hypnogram"])
            truncate_start_point = 3600 * 5
            truncate_end_point = (len(psg_edf)/100) - (3600 *5)
            annot.crop(truncate_start_point, truncate_end_point, verbose=False)
            psg_edf.set_annotations(annot, emit_warning=False)
            events, _ = mne.events_from_annotations(psg_edf, event_id=RANDK_LABEL2ID, chunk_duration=30., verbose=False)
            event_id = LABEL2ID
        else:
            start_psg_date = psg_edf.info["meas_date"]
            start_psg_date = start_psg_date.replace(tzinfo=None)
            test_start_time = sample_submission_df[sample_submission_df["id"]==row["id"]]["meas_time"].min()
            test_end_time = sample_submission_df[sample_submission_df["id"]==row["id"]]["meas_time"].max()
            truncate_start_point = int((test_start_time - start_psg_date).total_seconds())
            truncate_end_point = int((test_end_time- start_psg_date).total_seconds())+30
            event_range = list(range(truncate_start_point, truncate_end_point, 30))
            events = np.zeros((len(event_range), 3), dtype=int)
            events[:, 0] = event_range
            events = events * 100
            event_id = {'Sleep stage W': 0}
            
        tmax = 30. - 1. / psg_edf.info['sfreq']
        epoch = mne.Epochs(raw=psg_edf, events=events, event_id=event_id, tmin=0, tmax=tmax, baseline=None, verbose=False, on_missing='ignore')
        assert len(epoch.events) * 30 == truncate_end_point - truncate_start_point
        
        epoch.info["temp"] = {
            "id":row["id"],
            "subject_id":row["subject_id"],
            "night":row["night"],
            "age":row["age"],
            "sex":row["sex"],
            "truncate_start_point":truncate_start_point
        }
        whole_epoch_data.append(epoch)
    return whole_epoch_data

train_record_subset_df = train_record_df.sample(n=50).reset_index(drop=True)
train_subset_epoch = read_and_set_annoation(train_record_subset_df, include=["EEG Fpz-Cz"], is_test=False)
test_whole_epoch = read_and_set_annoation(test_record_df, include=["EEG Fpz-Cz"], is_test=True)

sample_events = train_subset_epoch[0].events[:, 2]
sample_epoch_df = train_subset_epoch[0].to_data_frame(verbose=False)

ID2LABEL = {v:k for k, v in LABEL2ID.items()}

go.Figure(
    data=[
        go.Scatter(x=sample_epoch_df["epoch"].unique(), y=list(map(lambda x: ID2LABEL[x], sample_events)))
    ],
    layout=go.Layout(
        yaxis=dict(title="sleep stage"),
        xaxis=dict(title="epoch"),
    )
)

epoch_grouped_df = sample_epoch_df.groupby("epoch").agg({"EEG Fpz-Cz":"mean"}).reset_index()

go.Figure(
    data=[
        go.Scatter(x=epoch_grouped_df["epoch"], y=epoch_grouped_df["EEG Fpz-Cz"]),
    ],
    layout=go.Layout(
        yaxis=dict(title="EEG Fpz-Cz"),
        xaxis=dict(title="epoch"),
    )
)

def eeg_power_band(epochs):
    FREQ_BANDS = {"delta": [0.5, 4.5],
                  "theta": [4.5, 8.5],
                  "alpha": [8.5, 11.5],
                  "sigma": [11.5, 15.5],
                  "beta": [15.5, 30]}
    spectrum = epochs.compute_psd(picks='eeg', fmin=0.5, fmax=30. ,verbose=False)
    psds, freqs = spectrum.get_data(return_freqs=True)
    psds /= np.sum(psds, axis=-1, keepdims=True)
    X = []
    for fmin, fmax in FREQ_BANDS.values():
        psds_band = psds[:, :, (freqs >= fmin) & (freqs < fmax)].mean(axis=-1)
        X.append(psds_band.reshape(len(psds), -1))
    return np.concatenate(X, axis=1)

train_df = []
for epoch in tqdm(train_subset_epoch):
    epoch_df = epoch_to_df(epoch)
    sub_df = epoch_to_sub_df(epoch_df, epoch.info["temp"]["id"], is_train=True)
    feature_df = pd.DataFrame(eeg_power_band(epoch))
    _df = pd.concat([sub_df, feature_df], axis=1)
    _df = _df[~_df["condition"].isin(["Sleep stage ?", "Movement time"])]
    train_df.append(_df)
train_df = pd.concat(train_df).reset_index(drop=True)
train_df["condition"].value_counts()
train_df["condition"] = train_df["condition"].map(LABEL2ID)
test_df = []

for epoch in tqdm(test_whole_epoch):
    epoch_df = epoch_to_df(epoch)
    sub_df = epoch_to_sub_df(epoch_df, epoch.info["temp"]["id"], is_train=False)
    feature_df = pd.DataFrame(eeg_power_band(epoch))
    _df = pd.concat([sub_df, feature_df], axis=1)
    test_df.append(pd.concat([sub_df, feature_df], axis=1))
test_df = pd.concat(test_df)

val_size = int(train_record_df["subject_id"].nunique() * 0.20)
train_all_subjects = train_record_df["subject_id"].unique()
np.random.shuffle(train_all_subjects)
val_subjects = train_all_subjects[:val_size]
val_ids = train_record_df[train_record_df["subject_id"].isin(val_subjects)]["id"]
trn_df = train_df[~train_df["id"].isin(val_ids)]
val_df = train_df[train_df["id"].isin(val_ids)]

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(trn_df.iloc[:, 3:], trn_df["condition"])

#joblib.dump(model, MODEL_DIR)
#model = joblib.load(MODEL_DIR)

print("\n==========学習済みモデルを保存しました。==========\n")

val_preds = model.predict(val_df.iloc[:, 3:])
score = accuracy_score(val_df["condition"], val_preds)
print("Accuracy Score："+str(score))
print(classification_report(val_df["condition"], val_preds,))

print("\n==========モデルの評価を完了しました==========\n")

test_df
test_preds = model.predict(test_df.iloc[:, 2:])
sample_submission_df["condition"] = test_preds
sample_submission_df["condition"] = sample_submission_df["condition"].map(ID2LABEL)
sample_submission_df
sample_submission_df["condition"].value_counts()
sample_submission_df.to_csv(OUTPUT_DIR, index=False)

print("\n==========プログラムを完了しました==========\n")